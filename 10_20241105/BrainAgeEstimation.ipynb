{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Subset\n",
        "import time\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "from monai.data import Dataset, DataLoader\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    LoadImaged,\n",
        "    EnsureChannelFirstd,\n",
        "    ScaleIntensityd\n",
        ")\n",
        "from monai.networks.nets import Regressor, ResNet, DenseNet121, DenseNet169, SEResNet50, SENet154, EfficientNetBN, ViT\n",
        "from monai.metrics import MAEMetric\n",
        "from monai.utils import first, set_determinism\n",
        "import shap\n",
        "import nibabel as nib\n",
        "from nilearn import plotting\n",
        "%env CUDA_VISIBLE_DEVICES=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Functions and Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, transforms=None):\n",
        "        self.data = data\n",
        "        self.transforms = transforms\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        data_transformed = self.transforms(self.data[idx]) if self.transforms else self.data[idx]\n",
        "        return data_transformed\n",
        "\n",
        "def load_data(data_dir, modalities, additional_variables, batch_size, test_size=0.2, inference=False):\n",
        "    df = pd.read_csv(os.path.join(data_dir, 'Subjects.csv'))\n",
        "    subjects = df['No'].apply(lambda x: f'{x:03d}').to_numpy()\n",
        "    data_dicts = []\n",
        "    for index, subject in enumerate(subjects):\n",
        "        subject_dict = {}\n",
        "        for modality in modalities:\n",
        "            subject_dict[modality] = os.path.join(data_dir, modality, f\"{subject}.nii.gz\")\n",
        "        if inference:\n",
        "            for variable in additional_variables:\n",
        "                if variable != 'Age':\n",
        "                    subject_dict[variable] = df[variable].to_numpy()[index]\n",
        "        else:\n",
        "            for variable in additional_variables:\n",
        "                subject_dict[variable] = df[variable].to_numpy()[index]\n",
        "        data_dicts.append(subject_dict)\n",
        "    img_transforms = Compose([\n",
        "        LoadImaged(keys=modalities, image_only=True),\n",
        "        EnsureChannelFirstd(keys=modalities),\n",
        "        ScaleIntensityd(keys=modalities, minv=0, maxv=1)\n",
        "    ])\n",
        "    ds = CustomDataset(data=data_dicts, transforms=img_transforms)\n",
        "    if inference:\n",
        "        test_loader = DataLoader(ds, batch_size=batch_size, num_workers=0, pin_memory=torch.cuda.is_available())\n",
        "        return test_loader, subjects\n",
        "    else:\n",
        "        train_ds, val_ds = train_test_split(ds, test_size=test_size, random_state=42)\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=torch.cuda.is_available())\n",
        "        val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=0, pin_memory=torch.cuda.is_available())\n",
        "        return train_loader, val_loader\n",
        "\n",
        "class ViTFeatureExtractor(nn.Module):\n",
        "    def __init__(self, input_channels, img_size, img_features=64):\n",
        "        super().__init__()\n",
        "        self.vit = ViT(\n",
        "            in_channels=input_channels,\n",
        "            img_size=img_size,\n",
        "            patch_size=(16, 16, 16),\n",
        "            classification=False,\n",
        "        )\n",
        "        self.fc = nn.Linear(768, img_features)\n",
        "    def forward(self, x):\n",
        "        vit_output = self.vit(x)\n",
        "        x = vit_output[0]\n",
        "        x = x[:, 0, :]\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class SFCN(nn.Module):\n",
        "    def __init__(self, input_channels=1, channel_number=[32, 64, 128, 256, 256, 64], output_dim=1, dropout=True):\n",
        "        super(SFCN, self).__init__()\n",
        "        n_layer = len(channel_number)\n",
        "        self.feature_extractor = nn.Sequential()\n",
        "        for i in range(n_layer):\n",
        "            if i == 0:\n",
        "                in_channel = input_channels\n",
        "            else:\n",
        "                in_channel = channel_number[i-1]\n",
        "            out_channel = channel_number[i]\n",
        "            if i < n_layer-1:\n",
        "                self.feature_extractor.add_module('conv_%d' % i,\n",
        "                                                  self.conv_layer(in_channel,\n",
        "                                                                  out_channel,\n",
        "                                                                  maxpool=True,\n",
        "                                                                  kernel_size=3,\n",
        "                                                                  padding=1))\n",
        "            else:\n",
        "                self.feature_extractor.add_module('conv_%d' % i,\n",
        "                                                  self.conv_layer(in_channel,\n",
        "                                                                  out_channel,\n",
        "                                                                  maxpool=False,\n",
        "                                                                  kernel_size=1,\n",
        "                                                                  padding=0))\n",
        "        self.regressor = nn.Sequential()\n",
        "        avg_shape = [4, 5, 4]\n",
        "        self.regressor.add_module('average_pool', nn.AvgPool3d(avg_shape))\n",
        "        if dropout is True:\n",
        "            self.regressor.add_module('dropout', nn.Dropout(0.5))\n",
        "        self.regressor.add_module('final_conv', nn.Conv3d(channel_number[-1], output_dim, padding=0, kernel_size=1))\n",
        "    @staticmethod\n",
        "    def conv_layer(in_channel, out_channel, maxpool=True, kernel_size=3, padding=0, maxpool_stride=2):\n",
        "        if maxpool is True:\n",
        "            layer = nn.Sequential(\n",
        "                nn.Conv3d(in_channel, out_channel, padding=padding, kernel_size=kernel_size),\n",
        "                nn.BatchNorm3d(out_channel),\n",
        "                nn.MaxPool3d(2, stride=maxpool_stride),\n",
        "                nn.ReLU(),\n",
        "            )\n",
        "        else:\n",
        "            layer = nn.Sequential(\n",
        "                nn.Conv3d(in_channel, out_channel, padding=padding, kernel_size=kernel_size),\n",
        "                nn.BatchNorm3d(out_channel),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "        return layer\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = self.regressor(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "class CNN3D(nn.Module):\n",
        "    def __init__(self, input_channels=1, output_dim=1):\n",
        "        super(CNN3D, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv1 = nn.Conv3d(input_channels, out_channels=16, kernel_size=6, padding=2)\n",
        "        self.pool = nn.MaxPool3d(2)\n",
        "        self.bn1 = nn.BatchNorm3d(16)\n",
        "        self.conv2 = nn.Conv3d(in_channels=16, out_channels=32, kernel_size=6, padding=2)\n",
        "        self.bn2 = nn.BatchNorm3d(32)\n",
        "        self.dropout1 = nn.Dropout3d(0.2)\n",
        "        self.conv3 = nn.Conv3d(in_channels=32, out_channels=64, kernel_size=6, padding=2)\n",
        "        self.bn3 = nn.BatchNorm3d(64)\n",
        "        self.dropout2 = nn.Dropout3d(0.2)\n",
        "        self.conv4 = nn.Conv3d(in_channels=64, out_channels=128, kernel_size=6, padding=2)\n",
        "        self.bn4 = nn.BatchNorm3d(128)\n",
        "        self.dropout3 = nn.Dropout3d(0.2)\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool3d(1)\n",
        "        self.fc1 = nn.Linear(128, 128)\n",
        "        self.dropout4 = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(128, output_dim)\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.relu(self.conv4(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout4(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "def get_model(model_name, input_channels, img_size, img_features):\n",
        "    if model_name == \"Regressor\":\n",
        "        return Regressor(\n",
        "            in_shape=[input_channels, *img_size],\n",
        "            out_shape=img_features,\n",
        "            channels=(16, 32, 64, 128, 256),\n",
        "            strides=(2, 2, 2, 2)\n",
        "        )\n",
        "    elif model_name == \"ResNet18\":\n",
        "        return ResNet(\n",
        "            block=\"basic\",\n",
        "            layers=[2, 2, 2, 2],\n",
        "            block_inplanes=[64, 128, 256, 512],\n",
        "            spatial_dims=3,\n",
        "            n_input_channels=input_channels,\n",
        "            num_classes=img_features\n",
        "        )\n",
        "    elif model_name == \"ResNet50\":\n",
        "        return ResNet(\n",
        "            block=\"bottleneck\",\n",
        "            layers=[3, 4, 6, 3],\n",
        "            block_inplanes=[64, 128, 256, 512],\n",
        "            spatial_dims=3,\n",
        "            n_input_channels=input_channels,\n",
        "            num_classes=img_features\n",
        "        )\n",
        "    elif model_name == \"DenseNet121\":\n",
        "        return DenseNet121(\n",
        "            spatial_dims=3,\n",
        "            in_channels=input_channels,\n",
        "            out_channels=img_features\n",
        "        )\n",
        "    elif model_name == \"DenseNet169\":\n",
        "        return DenseNet169(\n",
        "            spatial_dims=3,\n",
        "            in_channels=input_channels,\n",
        "            out_channels=img_features,\n",
        "        )\n",
        "    elif model_name == \"SEResNet50\":\n",
        "        return SEResNet50(\n",
        "            spatial_dims=3,\n",
        "            in_channels=input_channels,\n",
        "            num_classes=img_features\n",
        "        )\n",
        "    elif model_name == \"SENet154\":\n",
        "        return SENet154(\n",
        "            spatial_dims=3,\n",
        "            in_channels=input_channels,\n",
        "            num_classes=img_features\n",
        "        )\n",
        "    elif model_name == \"EfficientNetB0\":\n",
        "        return EfficientNetBN(\n",
        "            model_name=\"efficientnet-b0\",\n",
        "            spatial_dims=3,\n",
        "            in_channels=input_channels,\n",
        "            num_classes=img_features,\n",
        "        )\n",
        "    elif model_name == \"EfficientNetB2\":\n",
        "        return EfficientNetBN(\n",
        "            model_name=\"efficientnet-b2\",\n",
        "            spatial_dims=3,\n",
        "            in_channels=input_channels,\n",
        "            num_classes=img_features,\n",
        "        )\n",
        "    elif model_name == \"ViT\":\n",
        "        return ViTFeatureExtractor(\n",
        "            input_channels=input_channels,\n",
        "            img_size=img_size,\n",
        "            img_features=img_features\n",
        "        )\n",
        "    elif model_name == \"SFCN\":\n",
        "        return SFCN(\n",
        "            input_channels=input_channels,\n",
        "            channel_number=[32, 64, 128, 256, 256],\n",
        "            output_dim=img_features\n",
        "        )\n",
        "    elif model_name == \"CNN3D\":\n",
        "        return CNN3D(\n",
        "            input_channels=input_channels,\n",
        "            output_dim=img_features\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model name: {model_name}\")\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, channels=3, reduction=16):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        reduced_channel_size = max(channels // reduction, 1)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, reduced_channel_size, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(reduced_channel_size, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        b, c, _, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "class CustomRegressor(nn.Module):\n",
        "    def __init__(self, model_name, input_channels, img_size, img_features, additional_features=1, out_features=1):\n",
        "        super(CustomRegressor, self).__init__()\n",
        "        self.initial_attention = AttentionLayer(channels=input_channels)\n",
        "        self.img_regressor = get_model(model_name, input_channels, img_size, img_features)\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(img_features + additional_features, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, out_features))\n",
        "    def forward(self, x, y):\n",
        "        if x.size(1) > 1:\n",
        "            x = self.initial_attention(x)\n",
        "        x = self.img_regressor(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.cat((x, y), dim=1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "def train_one_epoch(model, device, train_loader, modalities, additional_variables, optimizer, criterion, scaler, metric):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    metric.reset()\n",
        "    for batch_data in train_loader:\n",
        "        targets = batch_data['Age'].unsqueeze(1).to(device)\n",
        "        images = [batch_data[modality].to(device) for modality in modalities]\n",
        "        img_inputs = torch.cat(images, dim=1)\n",
        "        variables = [batch_data[variable].view(-1,1).to(device) for variable in additional_variables if variable != 'Age']\n",
        "        variable_inputs = torch.cat(variables, dim=1)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(img_inputs, variable_inputs)\n",
        "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
        "            loss = criterion(outputs, targets)\n",
        "        if scaler:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        metric(y_pred=outputs, y=targets)\n",
        "    epoch_metric = metric.aggregate().item()\n",
        "    return epoch_loss / len(train_loader), epoch_metric\n",
        "\n",
        "def validate_one_epoch(model, device, val_loader, modalities, additional_variables, metric):\n",
        "    model.eval()\n",
        "    metric.reset()\n",
        "    with torch.no_grad():\n",
        "        for batch_data in val_loader:\n",
        "            targets = batch_data['Age'].unsqueeze(1).to(device)\n",
        "            images = [batch_data[modality].to(device) for modality in modalities]\n",
        "            img_inputs = torch.cat(images, dim=1)\n",
        "            variables = [batch_data[variable].view(-1,1).to(device) for variable in additional_variables if variable != 'Age']\n",
        "            variable_inputs = torch.cat(variables, dim=1)\n",
        "            outputs = model(img_inputs, variable_inputs)\n",
        "            metric(y_pred=outputs, y=targets)\n",
        "    return metric.aggregate().item()\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=30, delta=0):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.counter = 0\n",
        "    def __call__(self, metric):\n",
        "        score = metric\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "        elif score > self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "\n",
        "def train_model(model_dir, model, device, train_loader, val_loader, modalities, additional_variables, logger,\n",
        "        criterion, metric, max_epochs=100, learning_rate=1e-4, weight_decay=1e-5, val_interval=1, es_patience=30):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
        "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
        "    start_time = time.time()\n",
        "    best_metric = float(\"inf\")\n",
        "    best_metric_epoch = -1\n",
        "    early_stopping = EarlyStopping(patience=es_patience, delta=0)\n",
        "    epoch_loss_values, epoch_metric_values, metric_values = [], [], []\n",
        "    for epoch in range(max_epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        epoch_loss, epoch_metric = train_one_epoch(model, device, train_loader, modalities, additional_variables, optimizer, criterion, scaler, metric)\n",
        "        epoch_loss_values.append(epoch_loss)\n",
        "        epoch_metric_values.append(epoch_metric)\n",
        "        if (epoch + 1) % val_interval == 0:\n",
        "            val_metric = validate_one_epoch(model, device, val_loader, modalities, additional_variables, metric)\n",
        "            metric_values.append(val_metric)\n",
        "            if val_metric < best_metric:\n",
        "                best_metric = val_metric\n",
        "                best_metric_epoch = epoch + 1\n",
        "                best_model_state = model.state_dict()\n",
        "                torch.save(model.state_dict(), os.path.join(model_dir, \"BestMetricModel.pth\"))\n",
        "                logger.info(f\"Best MAE: {best_metric:.4f} at epoch {best_metric_epoch}\")\n",
        "            early_stopping(val_metric)\n",
        "            if early_stopping.early_stop:\n",
        "                logger.info(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
        "                print(f\"; Early stopping triggered at epoch {epoch + 1}\", end=\"\")\n",
        "                break\n",
        "        epoch_end_time = time.time()\n",
        "        logger.info(f\"Epoch {epoch + 1} computed for {(epoch_end_time - epoch_start_time)/60:.2f} mins - Training loss: {epoch_loss:.4f}, Training MAE: {epoch_metric:.4f}, Validation MAE: {val_metric:.4f}\")\n",
        "        lr_scheduler.step()\n",
        "        sys.stdout.write(f\"\\rEpoch {epoch + 1}/{max_epochs} completed\")\n",
        "        sys.stdout.flush()\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    logger.info(f\"Best MAE: {best_metric:.3f} at epoch {best_metric_epoch}; Total time consumed: {total_time/60:.2f} mins\")\n",
        "    print(f\"\\nBest MAE: {best_metric:.3f} at epoch {best_metric_epoch}; Total time consumed: {total_time/60:.2f} mins\")\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "    return model, epoch_loss_values, epoch_metric_values, metric_values\n",
        "\n",
        "def plot_metric_values(model_dir, epoch_loss_values, epoch_metric_values, metric_values, val_interval=1):\n",
        "    _, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    axs[0].plot( [i + 1 for i in range(len(epoch_loss_values))], epoch_loss_values, label='Training Loss', color='red')\n",
        "    axs[0].set_title('Training Loss')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_ylabel('Loss')\n",
        "    axs[1].plot([i + 1 for i in range(len(epoch_metric_values))], epoch_metric_values, label='Training MAE', color='red')\n",
        "    axs[1].plot([val_interval * (i + 1) for i in range(len(metric_values))], metric_values, label='Validation MAE', color='blue')\n",
        "    axs[1].set_title('Training MAE vs. Validation MAE')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_ylabel('MAE')\n",
        "    axs[1].legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(model_dir, \"Performance.png\"), dpi=300)\n",
        "\n",
        "class SHAP3D:\n",
        "    def __init__(self, model, modalities, additional_variables, device, batch_size):\n",
        "        self.model = model\n",
        "        self.modalities = modalities\n",
        "        self.additional_variables = additional_variables\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        self.explainer = None\n",
        "    def create_background(self, dl, num_samples=20):\n",
        "        dataset = dl.dataset\n",
        "        if num_samples is not None:\n",
        "            subset_indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
        "            subset_ds = Subset(dataset, subset_indices)\n",
        "            subset_dl = DataLoader(subset_ds, batch_size=self.batch_size, num_workers=0, pin_memory=torch.cuda.is_available())\n",
        "        else:\n",
        "            subset_dl = dl\n",
        "        background_images = []\n",
        "        background_variables = []\n",
        "        for batch_data in subset_dl:\n",
        "            images = [batch_data[modality].to(self.device) for modality in self.modalities]\n",
        "            img_inputs = torch.cat(images, dim=1)\n",
        "            variables = [batch_data[variable].view(-1,1).to(self.device) for variable in self.additional_variables if variable != 'Age']\n",
        "            variable_inputs = torch.cat(variables, dim=1)\n",
        "            background_images.append(img_inputs)\n",
        "            background_variables.append(variable_inputs)\n",
        "        background_images = torch.cat(background_images, dim=0)\n",
        "        background_variables = torch.cat(background_variables, dim=0)\n",
        "        self.explainer = shap.GradientExplainer(self.model, [background_images, background_variables])\n",
        "    def compute_shap_values(self, dl, num_samples=None):\n",
        "        dataset = dl.dataset\n",
        "        if num_samples is not None:\n",
        "            subset_indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
        "            subset_ds = Subset(dataset, subset_indices)\n",
        "            subset_dl = DataLoader(subset_ds, batch_size=self.batch_size, num_workers=0, pin_memory=torch.cuda.is_available())\n",
        "        else:\n",
        "            subset_dl = dl\n",
        "        img_shap_values_list = []\n",
        "        variable_shap_values_list = []\n",
        "        images_list = []\n",
        "        variables_list = []\n",
        "        for batch_data in subset_dl:\n",
        "            images = [batch_data[modality].to(self.device) for modality in self.modalities]\n",
        "            img_inputs = torch.cat(images, dim=1)\n",
        "            variables = [batch_data[variable].view(-1,1).to(self.device) for variable in self.additional_variables if variable != 'Age']\n",
        "            variable_inputs = torch.cat(variables, dim=1)\n",
        "            shap_values = self.explainer.shap_values([img_inputs, variable_inputs])\n",
        "            img_shap_values_list.append(shap_values[0])\n",
        "            variable_shap_values_list.append(shap_values[1])\n",
        "            images_list.append(img_inputs.cpu().numpy())\n",
        "            variables_list.append(variable_inputs.cpu().numpy())\n",
        "        self.img_shap_values = np.concatenate(img_shap_values_list, axis=0)\n",
        "        self.variable_shap_values = np.concatenate(variable_shap_values_list, axis=0)\n",
        "        self.images = np.concatenate(images_list, axis=0)\n",
        "        self.variables = np.concatenate(variables_list, axis=0)\n",
        "    def visualize_shap(self, sample_img_path, pred_dir, vmin=-0.0025, vmax=0.0025):\n",
        "        reference_img = nib.load(sample_img_path)\n",
        "        _, axs = plt.subplots(len(self.modalities), 1, figsize=(12, len(self.modalities) * 4))\n",
        "        if len(self.modalities) == 1:\n",
        "            axs = [axs]\n",
        "        for i, key in enumerate(self.modalities):\n",
        "            shap_values = self.img_shap_values[:, i, :, :, :, 0]\n",
        "            feature_values = self.images[:, i, :, :, :]\n",
        "            mean_abs_shap_values = np.mean(np.abs(shap_values), axis=0)\n",
        "            common_mask = np.all(feature_values != 0, axis=0)\n",
        "            masked_mean_abs_shap_values = np.zeros_like(mean_abs_shap_values)\n",
        "            masked_mean_abs_shap_values[common_mask] = mean_abs_shap_values[common_mask]\n",
        "            masked_mean_abs_shap_values_img = nib.Nifti1Image(masked_mean_abs_shap_values, \n",
        "                                                              affine=reference_img.affine, \n",
        "                                                              header=reference_img.header)\n",
        "            masked_mean_abs_shap_values_img.header['descrip'] = 'Mean absolute SHAP values'\n",
        "            nib.save(masked_mean_abs_shap_values_img, os.path.join(pred_dir, f\"MeanAbsSHAPValues_{key}.nii.gz\"))\n",
        "            plotting.plot_glass_brain(masked_mean_abs_shap_values_img, threshold=None, annotate=False,\n",
        "                                      plot_abs=False, black_bg='auto', axes=axs[i],\n",
        "                                      colorbar=True, cmap='black_red', symmetric_cbar=False,\n",
        "                                      alpha=0.3, vmin=vmin, vmax=vmax)\n",
        "            axs[i].set_title(f\"{key}\")\n",
        "        plt.show()\n",
        "        shap.summary_plot(self.variable_shap_values[:, :, 0], self.variables)\n",
        "\n",
        "def calculate_regression_parameters(model_dir, model, device, val_loader, modalities, additional_variables, metric, pred_dir):\n",
        "    model.load_state_dict(torch.load(os.path.join(model_dir, \"BestMetricModel.pth\")))\n",
        "    model.eval()\n",
        "    os.makedirs(pred_dir, exist_ok=True)\n",
        "    pred_values = []\n",
        "    target_values = []\n",
        "    metric.reset()\n",
        "    with torch.no_grad():\n",
        "        for batch_data in val_loader:\n",
        "            targets = batch_data['Age'].unsqueeze(1).to(device)\n",
        "            images = [batch_data[modality].to(device) for modality in modalities]\n",
        "            img_inputs = torch.cat(images, dim=1)\n",
        "            variables = [batch_data[variable].view(-1,1).to(device) for variable in additional_variables if variable != 'Age']\n",
        "            variable_inputs = torch.cat(variables, dim=1) if variables else None\n",
        "            outputs = model(img_inputs, variable_inputs) if variable_inputs is not None else model(img_inputs)\n",
        "            metric(y_pred=outputs, y=targets)\n",
        "            pred_values.extend(outputs.cpu().numpy().flatten())\n",
        "            target_values.extend(targets.cpu().numpy().flatten())\n",
        "        metric_value = metric.aggregate().item()\n",
        "    pred_values = np.array(pred_values)\n",
        "    target_values = np.array(target_values)\n",
        "    regression_param = np.polyfit(target_values, pred_values, 1)\n",
        "    slope, intercept = regression_param\n",
        "    r_value = np.corrcoef(target_values, pred_values)[0, 1]\n",
        "    df = pd.DataFrame({\n",
        "        'Slope': [slope],\n",
        "        'Intercept': [intercept],\n",
        "        'r': [r_value]\n",
        "    })\n",
        "    df.to_csv(os.path.join(pred_dir, \"RegressionParameter.csv\"), index=False)\n",
        "    corrected_pred_values = (pred_values - intercept) / slope\n",
        "    metric.reset()\n",
        "    with torch.no_grad():\n",
        "        y_pred = torch.tensor(corrected_pred_values, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "        y = torch.tensor(target_values, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "        metric(y_pred, y)\n",
        "        corrected_metric_value = metric.aggregate().item()\n",
        "    print(f'MAE on validation set: {metric_value:.3f}'\n",
        "          f'\\nCorrected MAE on validation set: {corrected_metric_value:.3f}')\n",
        "    return regression_param\n",
        "\n",
        "def apply_best_model(model_dir, model, device, test_loader, modalities, additional_variables,  pred_dir, subjects, regression_param=None):\n",
        "    model.load_state_dict(torch.load(os.path.join(model_dir, \"BestMetricModel.pth\")))\n",
        "    model.eval()\n",
        "    os.makedirs(pred_dir, exist_ok=True)\n",
        "    pred_values = np.array([])\n",
        "    with torch.no_grad():\n",
        "        for batch_data in test_loader:\n",
        "            images = [batch_data[modality].to(device) for modality in modalities]\n",
        "            img_inputs = torch.cat(images, dim=1)\n",
        "            variables = [batch_data[variable].view(-1,1).to(device) for variable in additional_variables if variable != 'Age']\n",
        "            variable_inputs = torch.cat(variables, dim=1)\n",
        "            outputs = model(img_inputs, variable_inputs)\n",
        "            pred_values = np.append(pred_values, outputs.cpu().numpy())\n",
        "    df = pd.DataFrame({\n",
        "        'No': subjects,\n",
        "        'BrainAge': pred_values\n",
        "    })\n",
        "    if regression_param is not None:\n",
        "        a, b = regression_param\n",
        "        df['CorrectedBrainAge'] = (df['BrainAge'] - b) / a\n",
        "    df.to_csv(os.path.join(pred_dir, \"BrainAge.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = \"BrainAgeEstimation_2mm\"\n",
        "model_dir_prefix = \"BrainAgeEstimation\"\n",
        "model_name = \"SFCN\" # any supported model name: Regressor, ResNet18, ResNet50, DenseNet121, DenseNet169, SEResNet50, SENet154, EfficientNetB0, EfficientNetB2, ViT, SFCN, CNN3D\n",
        "modalities = [\"GM\", \"FA\"]\n",
        "additional_variables = [\"Age\", \"Sex\"]\n",
        "test_size = 0.2\n",
        "batch_size = 5\n",
        "max_epochs = 100\n",
        "learning_rate = 1e-4\n",
        "weight_decay = 1e-5\n",
        "val_interval = 1\n",
        "es_patience = 30\n",
        "\n",
        "model_dir = f\"{model_dir_prefix}_{model_name}\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print('Device:', device)\n",
        "log_file = os.path.join(model_dir, \"Prediction.log\")\n",
        "logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(message)s\")\n",
        "logger = logging.getLogger()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_determinism(seed=0)\n",
        "train_loader, val_loader = load_data(os.path.join(data_dir, \"train\"), modalities, additional_variables, batch_size, test_size)\n",
        "\n",
        "# Check data shape\n",
        "tr = first(train_loader)\n",
        "img_size = tuple(tr[modalities[0]].shape[-3:])\n",
        "print('Data shape for training:')\n",
        "for key, value in tr.items():\n",
        "    print(f'\\u2022 {key}: {tuple(value.shape)} \\u00D7 {len(train_loader)}')\n",
        "vl = first(val_loader)\n",
        "print('\\nData shape for validation:')\n",
        "for key, value in vl.items():\n",
        "    print(f'\\u2022 {key}: {tuple(value.shape)} \\u00D7 {len(val_loader)}')\n",
        "\n",
        "# Visualize data\n",
        "_, axs = plt.subplots(1, len(modalities), figsize=(len(modalities) * 4, 5))\n",
        "slice_index = 40\n",
        "for i, key in enumerate(modalities):\n",
        "    image = tr[key][0, 0, :, :, :].detach().cpu()\n",
        "    img_slice = torch.rot90(image[:, :, slice_index], k=1, dims=(0, 1))\n",
        "    ax = axs[i]\n",
        "    ax.imshow(img_slice, cmap='gray')\n",
        "    ax.set_title(key)\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CustomRegressor(\n",
        "            model_name=model_name,\n",
        "            input_channels=len(modalities),\n",
        "            img_size=img_size,\n",
        "            img_features=64,\n",
        "            additional_features=1,\n",
        "            out_features=1\n",
        "        ).to(device)\n",
        "print(f\"Selected model: {model_name}\")\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "num_attentionalmodulator_params = sum(p.numel() for p in model.initial_attention.parameters() if p.requires_grad)\n",
        "num_featureextractor_params = sum(p.numel() for p in model.img_regressor.parameters() if p.requires_grad)\n",
        "num_regressor_params = sum(p.numel() for p in model.fc_layers.parameters() if p.requires_grad)\n",
        "print(f\"Number of trainable parameters: {num_params} = \"\n",
        "      f\"{num_attentionalmodulator_params} (attentional modulator) + \"\n",
        "      f\"{num_featureextractor_params} (feature extractor) + \"\n",
        "      f\"{num_regressor_params} (regressor)\")\n",
        "\n",
        "criterion = nn.L1Loss()\n",
        "metric = MAEMetric(reduction=\"mean\")\n",
        "model, epoch_loss_values, epoch_metric_values, metric_values = train_model(model_dir, model, device, train_loader, val_loader,\n",
        "    modalities, additional_variables, logger, criterion, metric, max_epochs, learning_rate, weight_decay, val_interval, es_patience)\n",
        "plot_metric_values(model_dir, epoch_loss_values, epoch_metric_values, metric_values, val_interval)\n",
        "\n",
        "# Visualize outcome\n",
        "sample_indices = [76, 64] # 36-year-old female, 100-year-old female\n",
        "slice_index = 40\n",
        "model.eval()\n",
        "fig, axs = plt.subplots(len(sample_indices), len(modalities), figsize=(len(modalities) * 4, len(sample_indices) * 5))\n",
        "titles = []\n",
        "for row, sample_index in enumerate(sample_indices):\n",
        "    with torch.no_grad():\n",
        "        target = torch.tensor(val_loader.dataset[sample_index]['Age']).unsqueeze(0).unsqueeze(1).to(device) # torch.tensor(val_loader.dataset[sample_index]['Age']).view(1, 1).to(device)\n",
        "        images = [val_loader.dataset[sample_index][modality].unsqueeze(0).to(device) for modality in modalities]\n",
        "        img_inputs = torch.cat(images, dim=1)\n",
        "        variables = [torch.tensor(val_loader.dataset[sample_index][variable]).view(1, -1).to(device) for variable in additional_variables if variable != 'Age']\n",
        "        variable_inputs = torch.cat(variables, dim=1)\n",
        "        output = model(img_inputs, variable_inputs)\n",
        "    for col, key in enumerate(modalities):\n",
        "        image = images[col][0, 0, :, :, :].detach().cpu()\n",
        "        img_slice = torch.rot90(image[:, :, slice_index], k=1, dims=(0, 1))\n",
        "        ax = axs[row, col]\n",
        "        ax.imshow(img_slice, cmap='gray')\n",
        "        ax.set_title(key)\n",
        "        ax.axis('off')\n",
        "    title = (\n",
        "        f'Sample {row + 1}: '\n",
        "        f'Chronological age = {target.item():.1f} yrs, '\n",
        "        f'Brain age = {output.item():.1f} yrs, '\n",
        "        f'BAG = {output.item() - target.item():.1f} yrs'\n",
        "    )\n",
        "    titles.append(title)\n",
        "combined_title = '\\n'.join(titles)\n",
        "plt.suptitle(combined_title, fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SHAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shap_analyzer = SHAP3D(model, modalities, additional_variables, device, batch_size)\n",
        "shap_analyzer.create_background(train_loader, num_samples=20)\n",
        "shap_analyzer.compute_shap_values(val_loader, num_samples=20)\n",
        "sample_img_path = os.path.join(data_dir, 'train', 'GM', '001.nii.gz')\n",
        "pred_dir = os.path.join(model_dir, \"Prediction\")\n",
        "vmin=0; vmax=0.01\n",
        "shap_analyzer.visualize_shap(sample_img_path, pred_dir, vmin, vmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_dir = os.path.join(model_dir, \"Prediction\")\n",
        "regression_param = calculate_regression_parameters(model_dir, model, device, val_loader, modalities, additional_variables, metric, pred_dir)\n",
        "test_loader, subjects = load_data(os.path.join(data_dir, \"test\"), modalities, additional_variables, batch_size, None, True)\n",
        "apply_best_model(model_dir, model, device, test_loader, modalities, additional_variables,  pred_dir, subjects, regression_param)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Anonymous",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
